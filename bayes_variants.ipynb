{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### BAYESIAN METHODS\n",
    "## Bernoulli Naive Bayes\n",
    "# Steps\n",
    "- Encode categorical columns using pd.getdummies(one-hot)\n",
    "- Create an instance of BernoulliNB with binarize = True so that all features are mapped to binary values\n",
    "- Simply fit the training data and then predict on the test data.\n",
    "- Scaling is not required in any of the below methods as no distance measure is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:318: UserWarning: The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.8833930564530341\n",
      "Validation accuracy 0.8847072645388682\n",
      "Best params are {'fit_prior': True, 'binarize': True}\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli Naive Bayes\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "\n",
    "to_encode = []\n",
    "for column in df.columns:\n",
    "    if column == 'LoanID':\n",
    "        continue\n",
    "    if df[column].dtype == 'object':\n",
    "        to_encode.append(column)\n",
    "    elif df[column].dtype not in ['float64', 'int64', 'float', 'int']:\n",
    "        to_encode.append(column)\n",
    "\n",
    "# for column in to_encode:\n",
    "#     le = LabelEncoder() #for now let's use labelEncoder\n",
    "#     # le = OneHotEncoder()\n",
    "#     print('column is ', column)\n",
    "#     df[column] = le.fit_transform(df[column])\n",
    "#     test_df[column] = le.transform(test_df[column])\n",
    "\n",
    "# one hot encoding\n",
    "df = pd.get_dummies(df, columns=to_encode, drop_first=True, dtype=int)\n",
    "test_df = pd.get_dummies(test_df, columns=to_encode, drop_first=True, dtype=int)\n",
    "print('encoding done')\n",
    "\n",
    "#turns out it doesn't make a difference with the new encoder\n",
    "\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "param_grid = {\n",
    "    \"binarize\": [True],\n",
    "    \"fit_prior\": [True, False]\n",
    "}\n",
    "bnb_clf = RandomizedSearchCV(estimator=bnb, param_distributions= param_grid)\n",
    "\n",
    "# bnb_clf = BernoulliNB(binarize=True, fit_prior=False)\n",
    "train_df, validation_df = train_test_split(df, test_size=0.25, random_state=17)\n",
    "# , stratify=df['Default'] if we add this somehow we get lower validation score\n",
    "\n",
    "train_df = train_df.drop(columns=['LoanID'])\n",
    "validation_df = validation_df.drop(columns=['LoanID'])\n",
    "\n",
    "x_train_df = train_df.drop(columns=['Default'])\n",
    "y_train_df = train_df['Default']\n",
    "\n",
    "\n",
    "bnb_clf.fit(x_train_df, y_train_df)\n",
    "\n",
    "x_validation_df = validation_df.drop(columns=['Default'])\n",
    "y_validation_df = validation_df['Default']\n",
    "\n",
    "y_validation_pred = bnb_clf.predict(x_validation_df)\n",
    "\n",
    "train_acc = accuracy_score(y_train_df, bnb_clf.predict(x_train_df))\n",
    "valid_acc = accuracy_score(y_validation_df, y_validation_pred)\n",
    "\n",
    "print(f'Training accuracy {train_acc}')\n",
    "print(f'Validation accuracy {valid_acc}')\n",
    "\n",
    "print(f'Best params are {bnb_clf.best_params_}')\n",
    "\n",
    "\n",
    "newdf = pd.DataFrame({\"LoanID\": test_df['LoanID'], \"Default\": bnb_clf.predict(X=test_df.drop(columns=['LoanID']))})\n",
    "newdf.to_csv('./csv_submissions/bernoulli_naive_bayes_out.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "- Similar steps like bernoulli except use GaussianNB this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
      "        Age  Income  LoanAmount  CreditScore  MonthsEmployed  NumCreditLines  \\\n",
      "156247   22  146848      122264          427             102               1   \n",
      "6305     43  106772      133332          342              16               3   \n",
      "201675   59   17794       31479          409             118               2   \n",
      "33277    29  114495        6845          395              67               4   \n",
      "74647    25  107260      204081          598              31               4   \n",
      "\n",
      "        InterestRate  LoanTerm  DTIRatio  Education  EmploymentType  \\\n",
      "156247         10.47        48      0.14          0               0   \n",
      "6305            5.05        60      0.17          0               2   \n",
      "201675         15.95        12      0.21          0               3   \n",
      "33277           6.74        36      0.71          1               0   \n",
      "74647           4.74        60      0.37          0               2   \n",
      "\n",
      "        MaritalStatus  HasMortgage  HasDependents  LoanPurpose  HasCoSigner  \\\n",
      "156247              0            0              0            4            1   \n",
      "6305                1            1              0            0            0   \n",
      "201675              1            0              1            2            0   \n",
      "33277               1            1              0            4            1   \n",
      "74647               2            0              0            0            1   \n",
      "\n",
      "        Default  \n",
      "156247        0  \n",
      "6305          0  \n",
      "201675        0  \n",
      "33277         0  \n",
      "74647         0  \n",
      "dissapear\n",
      "Training accuracy 0.6714314561776026\n",
      "Validation accuracy 0.671455844918739\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "categorical_cols.remove('LoanID') # we don't want to encode this\n",
    "\n",
    "print(categorical_cols)\n",
    "\n",
    "df = df.drop(columns=['LoanID'])\n",
    "\n",
    "ids = test_df['LoanID']\n",
    "test_df = test_df.drop(columns=['LoanID'])\n",
    "\n",
    "for column in categorical_cols:\n",
    "    le = LabelEncoder() #for now let's use labelEncoder\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    test_df[column] = le.transform(test_df[column])\n",
    "\n",
    "\n",
    "# df = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "# test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "\n",
    "\n",
    "# pt = PowerTransformer()\n",
    "# df = pt.fit_transform(df.drop(columns=['Default']))\n",
    "\n",
    "#both have been one hot encoded\n",
    "train_df, validation_df = train_test_split(df, test_size=0.2, random_state=17, stratify=df['Default'])\n",
    "\n",
    "print(train_df.head())\n",
    "print('dissapear')\n",
    "x_train_df = train_df.drop(columns=['Default'])\n",
    "y_train_df = train_df['Default']\n",
    "x_validation_df = validation_df.drop(columns=['Default'])\n",
    "y_validation_df = validation_df['Default']\n",
    "\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit_transform(x_train_df)\n",
    "# scaler.transform(x_validation_df)\n",
    "# scaler.transform(test_df)\n",
    "\n",
    "pt = PowerTransformer()\n",
    "x_train_df = pt.fit_transform(x_train_df)\n",
    "x_validation_df = pt.transform(x_validation_df)\n",
    "test_df = pt.transform(test_df)\n",
    "\n",
    "gnb_clf = GaussianNB(priors=[0.5, 0.5])\n",
    "gnb_clf.fit(x_train_df, y_train_df)\n",
    "\n",
    "\n",
    "y_validation_pred = gnb_clf.predict(x_validation_df)\n",
    "\n",
    "train_acc = accuracy_score(y_train_df, gnb_clf.predict(x_train_df))\n",
    "valid_acc = accuracy_score(y_validation_df, y_validation_pred)\n",
    "\n",
    "print(f'Training accuracy {train_acc}')\n",
    "print(f'Validation accuracy {valid_acc}')\n",
    "\n",
    "\n",
    "newdf = pd.DataFrame({\"LoanID\": ids, \"Default\": gnb_clf.predict(X=test_df)})\n",
    "newdf.to_csv('./csv_submissions/gaussian_naive_bayes_out.csv', index=False)\n",
    "\n",
    "# pretty obvious that this would perform terribly \n",
    "# as the data features don't form a gaussian distribution as seen in eda, \n",
    "# they form something more like a uniform distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:318: UserWarning: The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:993: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 982, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 415, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 764, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/naive_bayes.py\", line 102, in predict\n",
      "    jll = self._joint_log_likelihood(X)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/naive_bayes.py\", line 1513, in _joint_log_likelihood\n",
      "    jll += self.feature_log_prob_[i][:, indices].T\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "IndexError: index 249999 is out of bounds for axis 1 with size 249998\n",
      "\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:993: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 982, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 415, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 764, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/naive_bayes.py\", line 102, in predict\n",
      "    jll = self._joint_log_likelihood(X)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/naive_bayes.py\", line 1513, in _joint_log_likelihood\n",
      "    jll += self.feature_log_prob_[i][:, indices].T\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "IndexError: index 249999 is out of bounds for axis 1 with size 249998\n",
      "\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.9169120166721448\n",
      "Validation accuracy 0.5923895307094837\n",
      "best parameters are {'fit_prior': False}\n"
     ]
    }
   ],
   "source": [
    "# Categorical Naive Bayes\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "others = []\n",
    "for column in df.columns:\n",
    "    if column not in categorical_cols:\n",
    "        others.append(column)\n",
    "\n",
    "\n",
    "categorical_cols.remove('LoanID') # we don't want to encode this\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for column in others:\n",
    "#     req_mean = df[column].mean()\n",
    "#     def f1(el):\n",
    "#         if el >= req_mean:\n",
    "#             return 1\n",
    "#         else:\n",
    "#             return 0\n",
    "#     df[column] = df[column].apply(f1)\n",
    "\n",
    "print(categorical_cols)\n",
    "\n",
    "df = df.drop(columns='LoanID')\n",
    "\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "\n",
    "assert df.columns.all() == test_df.columns.all()\n",
    "\n",
    "\n",
    "#both have been one hot encoded\n",
    "train_df, validation_df = train_test_split(df, test_size=0.3, random_state=17)\n",
    "\n",
    "x_train_df = train_df.drop(columns=['Default'])\n",
    "y_train_df = train_df['Default']\n",
    "x_validation_df = validation_df.drop(columns=['Default'])\n",
    "y_validation_df = validation_df['Default']\n",
    "\n",
    "ids = test_df['LoanID']\n",
    "test_df = test_df.drop(columns=['LoanID'])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit_transform(x_train_df)\n",
    "# scaler.transform(x_validation_df)\n",
    "# scaler.transform(test_df)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"fit_prior\": [False, True],\n",
    "    # \"alpha\": [2.0, 5.0]\n",
    "}\n",
    "# cnb_clf = CategoricalNB(fit_prior=False)\n",
    "# cnb_clf.fit(x_train_df, y_train_df)\n",
    "\n",
    "cnb_clf = CategoricalNB()\n",
    "rs = RandomizedSearchCV(cnb_clf, param_distributions=param_grid, cv=10)\n",
    "\n",
    "rs.fit(x_train_df, y_train_df)\n",
    "y_validation_pred = rs.predict(x_validation_df)\n",
    "\n",
    "train_acc = accuracy_score(y_train_df, rs.predict(x_train_df))\n",
    "valid_acc = accuracy_score(y_validation_df, y_validation_pred)\n",
    "\n",
    "print(f'Training accuracy {train_acc}')\n",
    "print(f'Validation accuracy {valid_acc}')\n",
    "\n",
    "print('best parameters are', rs.best_params_)\n",
    "\n",
    "newdf = pd.DataFrame({\"LoanID\": ids, \"Default\": rs.predict(X=test_df)})\n",
    "newdf.to_csv('categorical_naive_bayes_out.csv', index=False)\n",
    "\n",
    "# this also doesn't work that well as we have categorical columns mixed with continuous columns like income and it overfits training data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
